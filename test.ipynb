{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# data = pd.read_csv('your_dataset.csv')  # Assuming a CSV with 'paragraph' and 'dialogue' columns\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparagraph\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m tokenizer(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Model initialization\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = \"\"\"John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\"\"\"\n",
    "# data = pd.read_csv('your_dataset.csv')  # Assuming a CSV with 'paragraph' and 'dialogue' columns\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "inputs = tokenizer(data['paragraph'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "labels = tokenizer(data['dialogue'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Model initialization\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=inputs,  # Make sure to create a proper dataset class\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Generate dialogue from a new paragraph\n",
    "def generate_dialogue(paragraph):\n",
    "    input_ids = tokenizer(paragraph, return_tensors='pt').input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "print(generate_dialogue(\"Your paragraph here\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Dialogue:\n",
      "Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with refined few-shot examples and clear instructions\n",
    "examples = \"\"\"\n",
    "Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=250,\n",
    "        num_beams=4,  # For diverse generation\n",
    "        repetition_penalty=1.2,  # To reduce repetitive text\n",
    "        temperature=0.7,  # To control randomness\n",
    "        length_penalty=1.0,  # To ensure reasonable output length\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Post-process the output to clean and structure it\n",
    "def clean_dialogue_output(raw_output):\n",
    "    lines = raw_output.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if ':' in line and line.strip():  # Keep valid dialogue lines\n",
    "            cleaned_lines.append(line.strip())\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "processed_output = clean_dialogue_output(raw_output)\n",
    "\n",
    "# Print the processed dialogue\n",
    "print(\"Generated Dialogue:\")\n",
    "print(processed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['echo', 'Generated Dialogue:\\nConvert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It\\'s complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don\\'t know what to do.\"'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import subprocess\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with refined few-shot examples and clear instructions\n",
    "examples = \"\"\"\n",
    "Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=250,\n",
    "        num_beams=4,  # For diverse generation\n",
    "        repetition_penalty=1.2,  # To reduce repetitive text\n",
    "        temperature=0.7,  # To control randomness\n",
    "        length_penalty=1.0,  # To ensure reasonable output length\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Post-process the output to clean and structure it\n",
    "def clean_dialogue_output(raw_output):\n",
    "    lines = raw_output.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if ':' in line and line.strip():  # Keep valid dialogue lines\n",
    "            cleaned_lines.append(line.strip())\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "processed_output = clean_dialogue_output(raw_output)\n",
    "\n",
    "# Write the processed output to the terminal\n",
    "subprocess.run([\"echo\", f\"Generated Dialogue:\\n{processed_output}\"], shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Dialogue:\n",
      "\n",
      "Raw Output:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def generate_dialogue(narrative_text):\n",
    "    # Initialize the T5 tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "    # Prepare the prompt with clear instructions\n",
    "    prompt = f\"\"\"\n",
    "    Convert the following narrative into a structured dialogue with speaker names:\n",
    "    {narrative_text}\n",
    "    Dialogue:\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=512).input_ids\n",
    "\n",
    "    # Generate dialogue\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=256,  # Adjust based on the expected length of the output\n",
    "        num_beams=5,     # Beam search for better results\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-process the output to extract clean dialogue\n",
    "    def clean_dialogue_output(raw_output):\n",
    "        lines = raw_output.splitlines()\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            if ':' in line and line.strip():  # Retain only valid dialogue lines\n",
    "                cleaned_lines.append(line.strip())\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return clean_dialogue_output(raw_output)\n",
    "\n",
    "# Test the function with your narrative\n",
    "narrative = \"\"\"\n",
    "John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said, \"Everything will be okay. We'll figure this out together.\"\n",
    "John smiled weakly and said, \"I don't know what to do,\" his voice trembling.\n",
    "Mary replied, \"We will start looking for new opportunities tomorrow.\"\n",
    "\"\"\"\n",
    "\n",
    "# Generate dialogue\n",
    "dialogue = generate_dialogue(narrative)\n",
    "print(\"Generated Dialogue:\")\n",
    "print(dialogue)\n",
    "print(\"Raw Output:\")\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Dialogue:\n",
      "\n",
      "Raw Output Tokens: tensor([[    0, 10998,     1]])\n",
      "Decoded Raw Output: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def generate_dialogue(narrative_text):\n",
    "    # Initialize the T5 tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "    # Prepare the prompt with clear instructions\n",
    "    prompt = \"\"\"\n",
    "Convert the following narrative into a structured dialogue with speaker names:\n",
    "\n",
    "Example:\n",
    "Narrative: Jane walked into the room and said hello to Mike. Mike replied, \"Hello, Jane! How have you been?\"\n",
    "Dialogue:\n",
    "Jane: \"Hello, Mike.\"\n",
    "Mike: \"Hello, Jane! How have you been?\"\n",
    "\n",
    "Narrative:\n",
    "John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said, \"Everything will be okay. We'll figure this out together.\"\n",
    "John smiled weakly and said, \"I don't know what to do,\" his voice trembling.\n",
    "Mary replied, \"We will start looking for new opportunities tomorrow.\"\n",
    "\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=512).input_ids\n",
    "\n",
    "    # Generate dialogue\n",
    "    outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=256,\n",
    "    num_beams=5,  # Increase for higher-quality results\n",
    "    repetition_penalty=2.0,  # Penalize repetitive sequences\n",
    "    temperature=0.9,  # Slightly increase creativity\n",
    "    length_penalty=1.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "    # Decode the output\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-process the output to extract clean dialogue\n",
    "    def clean_dialogue_output(raw_output):\n",
    "        lines = raw_output.splitlines()\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            if ':' in line and line.strip():  # Retain only valid dialogue lines\n",
    "                cleaned_lines.append(line.strip())\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return clean_dialogue_output(raw_output)\n",
    "\n",
    "# Test the function with your narrative\n",
    "narrative = \"\"\"\n",
    "Narrative: Sarah walked up to Tom with a worried expression. She asked, \"What happened today?\" \n",
    "Tom hesitated and then replied, \"It's complicated.\"\n",
    "\"\"\"\n",
    "# Generate dialogue\n",
    "dialogue = generate_dialogue(narrative)\n",
    "print(\"Generated Dialogue:\")\n",
    "print(dialogue)\n",
    "print(\"Raw Output Tokens:\", outputs)\n",
    "print(\"Decoded Raw Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\abhis\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m processed_output\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Extract the dialogue text\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m dialogue_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Dialogue:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split the dialogue text into individual lines\u001b[39;00m\n\u001b[0;32m     82\u001b[0m lines \u001b[38;5;241m=\u001b[39m dialogue_text\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Your code to generate dialogue here\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "lines = raw_output.splitlines()\n",
    "\n",
    "# Remove duplicate lines and empty lines\n",
    "unique_lines = []\n",
    "for line in lines:\n",
    "    if line not in unique_lines and line.strip():\n",
    "        unique_lines.append(line)\n",
    "\n",
    "# Combine the cleaned lines\n",
    "processed_output = \"\\n\".join(unique_lines)\n",
    "\n",
    "# Print the processed dialogue\n",
    "# print(\"Generated Dialogue:\")\n",
    "# print(processed_output)\n",
    "\n",
    "# output = \"\"\"Generated Dialogue:\n",
    "# Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\"\"\"\n",
    "output = processed_output\n",
    "# Extract the dialogue text\n",
    "dialogue_text = output.split(\"Generated Dialogue:\")[1].strip()\n",
    "\n",
    "# Split the dialogue text into individual lines\n",
    "lines = dialogue_text.splitlines()\n",
    "\n",
    "# Initialize an empty list to store the formatted dialogue\n",
    "formatted_dialogue = []\n",
    "\n",
    "# Iterate through the lines and format the dialogue\n",
    "for line in lines:\n",
    "    if line.startswith(\"Narrative:\"):\n",
    "        continue\n",
    "    elif line.startswith(\"Dialogue:\"):\n",
    "        continue\n",
    "    else:\n",
    "        # Extract the speaker name and quote\n",
    "        speaker_name = line.split(\":\")[0].strip()\n",
    "        quote = line.split(\":\")[1].strip().strip('\"')\n",
    "        \n",
    "        # Format the dialogue\n",
    "        formatted_dialogue.append(f\"{speaker_name}: \\\"{quote}\\\"\")\n",
    "\n",
    "# Join the formatted dialogue into a single string\n",
    "formatted_dialogue_str = \"\\n\".join(formatted_dialogue)\n",
    "\n",
    "print(formatted_dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n",
      "\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m processed_output\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Extract the dialogue text\u001b[39;00m\n",
      "\u001b[1;32m---> 79\u001b[0m dialogue_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Dialogue:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split the dialogue text into individual lines\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m lines \u001b[38;5;241m=\u001b[39m dialogue_text\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Your code to generate dialogue here\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "lines = raw_output.splitlines()\n",
    "\n",
    "# Remove duplicate lines and empty lines\n",
    "unique_lines = []\n",
    "for line in lines:\n",
    "    if line not in unique_lines and line.strip():\n",
    "        unique_lines.append(line)\n",
    "\n",
    "# Combine the cleaned lines\n",
    "processed_output = \"\\n\".join(unique_lines)\n",
    "\n",
    "# Print the processed dialogue\n",
    "# print(\"Generated Dialogue:\")\n",
    "# print(processed_output)\n",
    "\n",
    "# output = \"\"\"Generated Dialogue:\n",
    "# Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\"\"\"\n",
    "output = processed_output\n",
    "# Extract the dialogue text\n",
    "dialogue_text = output.split(\"Generated Dialogue:\")[1].strip()\n",
    "\n",
    "# Split the dialogue text into individual lines\n",
    "lines = dialogue_text.splitlines()\n",
    "\n",
    "# Initialize an empty list to store the formatted dialogue\n",
    "formatted_dialogue = []\n",
    "\n",
    "# Iterate through the lines and format the dialogue\n",
    "for line in lines:\n",
    "    if line.startswith(\"Narrative:\"):\n",
    "        continue\n",
    "    elif line.startswith(\"Dialogue:\"):\n",
    "        continue\n",
    "    else:\n",
    "        # Extract the speaker name and quote\n",
    "        speaker_name = line.split(\":\")[0].strip()\n",
    "        quote = line.split(\":\")[1].strip().strip('\"')\n",
    "        \n",
    "        # Format the dialogue\n",
    "        formatted_dialogue.append(f\"{speaker_name}: \\\"{quote}\\\"\")\n",
    "\n",
    "# Join the formatted dialogue into a single string\n",
    "formatted_dialogue_str = \"\\n\".join(formatted_dialogue)\n",
    "\n",
    "print(formatted_dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n",
      "\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m processed_output\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Extract the dialogue text\u001b[39;00m\n",
      "\u001b[1;32m---> 79\u001b[0m dialogue_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Dialogue:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split the dialogue text into individual lines\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m lines \u001b[38;5;241m=\u001b[39m dialogue_text\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Your code to generate dialogue here\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "lines = raw_output.splitlines()\n",
    "\n",
    "# Remove duplicate lines and empty lines\n",
    "unique_lines = []\n",
    "for line in lines:\n",
    "    if line not in unique_lines and line.strip():\n",
    "        unique_lines.append(line)\n",
    "\n",
    "# Combine the cleaned lines\n",
    "processed_output = \"\\n\".join(unique_lines)\n",
    "\n",
    "# Print the processed dialogue\n",
    "# print(\"Generated Dialogue:\")\n",
    "# print(processed_output)\n",
    "\n",
    "# output = \"\"\"Generated Dialogue:\n",
    "# Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\"\"\"\n",
    "output = processed_output\n",
    "# Extract the dialogue text\n",
    "dialogue_text = output.split(\"Generated Dialogue:\")[1].strip()\n",
    "\n",
    "# Split the dialogue text into individual lines\n",
    "lines = dialogue_text.splitlines()\n",
    "\n",
    "# Initialize an empty list to store the formatted dialogue\n",
    "formatted_dialogue = []\n",
    "\n",
    "# Iterate through the lines and format the dialogue\n",
    "for line in lines:\n",
    "    if line.startswith(\"Narrative:\"):\n",
    "        continue\n",
    "    elif line.startswith(\"Dialogue:\"):\n",
    "        continue\n",
    "    else:\n",
    "        # Extract the speaker name and quote\n",
    "        speaker_name = line.split(\":\")[0].strip()\n",
    "        quote = line.split(\":\")[1].strip().strip('\"')\n",
    "        \n",
    "        # Format the dialogue\n",
    "        formatted_dialogue.append(f\"{speaker_name}: \\\"{quote}\\\"\")\n",
    "\n",
    "# Join the formatted dialogue into a single string\n",
    "formatted_dialogue_str = \"\\n\".join(formatted_dialogue)\n",
    "\n",
    "print(formatted_dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n",
      "\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m processed_output\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Extract the dialogue text\u001b[39;00m\n",
      "\u001b[1;32m---> 79\u001b[0m dialogue_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Dialogue:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split the dialogue text into individual lines\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m lines \u001b[38;5;241m=\u001b[39m dialogue_text\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Your code to generate dialogue here\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "lines = raw_output.splitlines()\n",
    "\n",
    "# Remove duplicate lines and empty lines\n",
    "unique_lines = []\n",
    "for line in lines:\n",
    "    if line not in unique_lines and line.strip():\n",
    "        unique_lines.append(line)\n",
    "\n",
    "# Combine the cleaned lines\n",
    "processed_output = \"\\n\".join(unique_lines)\n",
    "\n",
    "# Print the processed dialogue\n",
    "# print(\"Generated Dialogue:\")\n",
    "# print(processed_output)\n",
    "\n",
    "# output = \"\"\"Generated Dialogue:\n",
    "# Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\"\"\"\n",
    "output = processed_output\n",
    "# Extract the dialogue text\n",
    "dialogue_text = output.split(\"Generated Dialogue:\")[1].strip()\n",
    "\n",
    "# Split the dialogue text into individual lines\n",
    "lines = dialogue_text.splitlines()\n",
    "\n",
    "# Initialize an empty list to store the formatted dialogue\n",
    "formatted_dialogue = []\n",
    "\n",
    "# Iterate through the lines and format the dialogue\n",
    "for line in lines:\n",
    "    if line.startswith(\"Narrative:\"):\n",
    "        continue\n",
    "    elif line.startswith(\"Dialogue:\"):\n",
    "        continue\n",
    "    else:\n",
    "        # Extract the speaker name and quote\n",
    "        speaker_name = line.split(\":\")[0].strip()\n",
    "        quote = line.split(\":\")[1].strip().strip('\"')\n",
    "        \n",
    "        # Format the dialogue\n",
    "        formatted_dialogue.append(f\"{speaker_name}: \\\"{quote}\\\"\")\n",
    "\n",
    "# Join the formatted dialogue into a single string\n",
    "formatted_dialogue_str = \"\\n\".join(formatted_dialogue)\n",
    "\n",
    "print(formatted_dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n",
      "\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m processed_output\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Extract the dialogue text\u001b[39;00m\n",
      "\u001b[1;32m---> 79\u001b[0m dialogue_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Dialogue:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split the dialogue text into individual lines\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m lines \u001b[38;5;241m=\u001b[39m dialogue_text\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Your code to generate dialogue here\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "lines = raw_output.splitlines()\n",
    "\n",
    "# Remove duplicate lines and empty lines\n",
    "unique_lines = []\n",
    "for line in lines:\n",
    "    if line not in unique_lines and line.strip():\n",
    "        unique_lines.append(line)\n",
    "\n",
    "# Combine the cleaned lines\n",
    "processed_output = \"\\n\".join(unique_lines)\n",
    "\n",
    "# Print the processed dialogue\n",
    "# print(\"Generated Dialogue:\")\n",
    "# print(processed_output)\n",
    "\n",
    "# output = \"\"\"Generated Dialogue:\n",
    "# Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\"\"\"\n",
    "output = processed_output\n",
    "# Extract the dialogue text\n",
    "dialogue_text = output.split(\"Generated Dialogue:\")[1].strip()\n",
    "\n",
    "# Split the dialogue text into individual lines\n",
    "lines = dialogue_text.splitlines()\n",
    "\n",
    "# Initialize an empty list to store the formatted dialogue\n",
    "formatted_dialogue = []\n",
    "\n",
    "# Iterate through the lines and format the dialogue\n",
    "for line in lines:\n",
    "    if line.startswith(\"Narrative:\"):\n",
    "        continue\n",
    "    elif line.startswith(\"Dialogue:\"):\n",
    "        continue\n",
    "    else:\n",
    "        # Extract the speaker name and quote\n",
    "        speaker_name = line.split(\":\")[0].strip()\n",
    "        quote = line.split(\":\")[1].strip().strip('\"')\n",
    "        \n",
    "        # Format the dialogue\n",
    "        formatted_dialogue.append(f\"{speaker_name}: \\\"{quote}\\\"\")\n",
    "\n",
    "# Join the formatted dialogue into a single string\n",
    "formatted_dialogue_str = \"\\n\".join(formatted_dialogue)\n",
    "\n",
    "print(formatted_dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n",
      "\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m processed_output\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Extract the dialogue text\u001b[39;00m\n",
      "\u001b[1;32m---> 79\u001b[0m dialogue_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Dialogue:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split the dialogue text into individual lines\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m lines \u001b[38;5;241m=\u001b[39m dialogue_text\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Your code to generate dialogue here\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "lines = raw_output.splitlines()\n",
    "\n",
    "# Remove duplicate lines and empty lines\n",
    "unique_lines = []\n",
    "for line in lines:\n",
    "    if line not in unique_lines and line.strip():\n",
    "        unique_lines.append(line)\n",
    "\n",
    "# Combine the cleaned lines\n",
    "processed_output = \"\\n\".join(unique_lines)\n",
    "\n",
    "# Print the processed dialogue\n",
    "# print(\"Generated Dialogue:\")\n",
    "# print(processed_output)\n",
    "\n",
    "# output = \"\"\"Generated Dialogue:\n",
    "# Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\"\"\"\n",
    "output = processed_output\n",
    "# Extract the dialogue text\n",
    "dialogue_text = output.split(\"Generated Dialogue:\")[1].strip()\n",
    "\n",
    "# Split the dialogue text into individual lines\n",
    "lines = dialogue_text.splitlines()\n",
    "\n",
    "# Initialize an empty list to store the formatted dialogue\n",
    "formatted_dialogue = []\n",
    "\n",
    "# Iterate through the lines and format the dialogue\n",
    "for line in lines:\n",
    "    if line.startswith(\"Narrative:\"):\n",
    "        continue\n",
    "    elif line.startswith(\"Dialogue:\"):\n",
    "        continue\n",
    "    else:\n",
    "        # Extract the speaker name and quote\n",
    "        speaker_name = line.split(\":\")[0].strip()\n",
    "        quote = line.split(\":\")[1].strip().strip('\"')\n",
    "        \n",
    "        # Format the dialogue\n",
    "        formatted_dialogue.append(f\"{speaker_name}: \\\"{quote}\\\"\")\n",
    "\n",
    "# Join the formatted dialogue into a single string\n",
    "formatted_dialogue_str = \"\\n\".join(formatted_dialogue)\n",
    "\n",
    "print(formatted_dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n",
      "\u001b[0;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m processed_output\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Extract the dialogue text\u001b[39;00m\n",
      "\u001b[1;32m---> 79\u001b[0m dialogue_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Dialogue:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Split the dialogue text into individual lines\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m lines \u001b[38;5;241m=\u001b[39m dialogue_text\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Your code to generate dialogue here\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"Convert the following **Narrative** into a structured **Dialogue** with speaker names:\n",
    "Example 1:\n",
    "Narrative: Jane entered the room and smiled warmly at Mike. \"Good to see you,\" she said. Mike grinned back. \"Same here.\"\n",
    "Dialogue:\n",
    "Jane: \"Good to see you.\"\n",
    "Mike: \"Same here.\"\n",
    "\n",
    "Example 2:\n",
    "Narrative: Sarah walked up to Tom with a worried expression. \"What happened today?\" she asked. Tom hesitated, then sighed. \"It's complicated.\"\n",
    "Dialogue:\n",
    "Sarah: \"What happened today?\"\n",
    "Tom: \"It's complicated.\"\n",
    "\n",
    "Now convert this:\n",
    "Narrative: John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "Dialogue:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode and clean the generated tokens\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "lines = raw_output.splitlines()\n",
    "\n",
    "# Remove duplicate lines and empty lines\n",
    "unique_lines = []\n",
    "for line in lines:\n",
    "    if line not in unique_lines and line.strip():\n",
    "        unique_lines.append(line)\n",
    "\n",
    "# Combine the cleaned lines\n",
    "processed_output = \"\\n\".join(unique_lines)\n",
    "\n",
    "# Print the processed dialogue\n",
    "# print(\"Generated Dialogue:\")\n",
    "# print(processed_output)\n",
    "\n",
    "# output = \"\"\"Generated Dialogue:\n",
    "# Convert the following **Narrative** into a structured **Dialogue** with speaker names: Example 1: Narrative: Jane smiled warmly at Mike. \"Good to see you.\" Dialogue: Sarah walked up to Tom with a worried expression. \"It's complicated.\" Example 3: Narrative: John smiled weakly and sat down on the couch. \"I don't know what to do.\"\"\"\"\n",
    "output = processed_output\n",
    "# Extract the dialogue text\n",
    "dialogue_text = output.split(\"Generated Dialogue:\")[1].strip()\n",
    "\n",
    "# Split the dialogue text into individual lines\n",
    "lines = dialogue_text.splitlines()\n",
    "\n",
    "# Initialize an empty list to store the formatted dialogue\n",
    "formatted_dialogue = []\n",
    "\n",
    "# Iterate through the lines and format the dialogue\n",
    "for line in lines:\n",
    "    if line.startswith(\"Narrative:\"):\n",
    "        continue\n",
    "    elif line.startswith(\"Dialogue:\"):\n",
    "        continue\n",
    "    else:\n",
    "        # Extract the speaker name and quote\n",
    "        speaker_name = line.split(\":\")[0].strip()\n",
    "        quote = line.split(\":\")[1].strip().strip('\"')\n",
    "        \n",
    "        # Format the dialogue\n",
    "        formatted_dialogue.append(f\"{speaker_name}: \\\"{quote}\\\"\")\n",
    "\n",
    "# Join the formatted dialogue into a single string\n",
    "formatted_dialogue_str = \"\\n\".join(formatted_dialogue)\n",
    "\n",
    "print(formatted_dialogue_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue format not recognized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`do_sample` is set to `False`\")\n",
    "\n",
    "# Set logging to suppress unnecessary information\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Initialize the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Define the input narrative text with few-shot examples\n",
    "examples = \"\"\"John walked into the dimly lit room, his shoulders slumped. He told Mary that he had lost his job today. \n",
    "Mary's eyes widened with concern. She said that everything would be okay. \"We'll figure this out together,\" she assured him warmly.\n",
    "John smiled weakly and sat down on the couch. \"I don't know what to do,\" he whispered, his voice trembling.\n",
    "Mary sat beside him and told him that they would start looking for new opportunities tomorrow.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare input for the T5 model\n",
    "input_ids = tokenizer(examples, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Generate dialogue using the T5 model with max_length to avoid truncation warning\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=250,  # Adjust max_length based on the expected length of output\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens into readable text\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Check if the model's output is in the correct format\n",
    "if \"Dialogue:\" not in raw_output:\n",
    "    print(\"Dialogue format not recognized.\")\n",
    "else:\n",
    "    print(\"Generated Dialogue:\")\n",
    "    print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9.91M/9.91M [00:03<00:00, 3.21MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28.9k/28.9k [00:00<00:00, 561kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1.65M/1.65M [00:00<00:00, 2.64MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4.54k/4.54k [00:00<00:00, 4.21MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Epoch 1, Loss: 0.3908133572384493\n",
      "Epoch 2, Loss: 0.19090443029443718\n",
      "Epoch 3, Loss: 0.1387457896405255\n",
      "Epoch 4, Loss: 0.10946547301478192\n",
      "Epoch 5, Loss: 0.0930230775639526\n",
      "Accuracy: 96.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1. Load MNIST Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 2. Define the Model (Simple Feedforward Neural Network)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Flattened MNIST images (28x28)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # 10 classes for digits 0-9\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the image\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation function\n",
    "        x = torch.relu(self.fc2(x))  # Apply ReLU activation function\n",
    "        x = self.fc3(x)  # No activation at the final layer (for classification)\n",
    "        return x\n",
    "\n",
    "# 3. Initialize Model, Loss Function, and Optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Train the Model\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\thinc\\config.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\thinc\\types.py:25\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     Any,\n\u001b[0;32m      6\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     overload,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[0;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\thinc\\compat.py:96\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py:45\u001b[0m\n\u001b[0;32m     36\u001b[0m     _warn((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py is running against HDF5 \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m when it was built against \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis may cause problems\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_version_tuple),\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple)\n\u001b[0;32m     40\u001b[0m     ))\n\u001b[0;32m     43\u001b[0m _errors\u001b[38;5;241m.\u001b[39msilence_errors()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_converters \u001b[38;5;28;01mas\u001b[39;00m _register_converters, \\\n\u001b[0;32m     46\u001b[0m                    unregister_converters \u001b[38;5;28;01mas\u001b[39;00m _unregister_converters\n\u001b[0;32m     47\u001b[0m _register_converters()\n\u001b[0;32m     48\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(_unregister_converters)\n",
      "File \u001b[1;32mh5py\\\\_conv.pyx:1\u001b[0m, in \u001b[0;36minit h5py._conv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5r.pyx:1\u001b[0m, in \u001b[0;36minit h5py.h5r\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5p.pyx:1\u001b[0m, in \u001b[0;36minit h5py.h5p\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Sample narrative text (for simplicity, you can use your own dataset)\n",
    "narrative_text = \"\"\"\n",
    "Sarah was walking down the street when she suddenly bumped into James. \n",
    "\"Oh, sorry!\" she exclaimed. \n",
    "James smiled and said, \"No problem, Sarah.\" \n",
    "\"How have you been?\" Sarah asked, still a bit embarrassed. \n",
    "\"I'm doing great! Just busy with work,\" James replied. \n",
    "\"That's good to hear!\" she said with a smile.\n",
    "\"\"\"\n",
    "\n",
    "# Load the spaCy model for named entity recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Identify sentences using regex or NLP library (spaCy can be used here for sentence segmentation)\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sentences.append(sent.text.strip())\n",
    "    return sentences\n",
    "\n",
    "# Preprocess the narrative text into sentences\n",
    "sentences = preprocess_text(narrative_text)\n",
    "\n",
    "# Print the sentences\n",
    "for sent in sentences:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sample narrative text (for simplicity, you can use your own dataset)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:2475\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m Downloader()\n\u001b[0;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_download_dir()\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:1068\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample narrative text (for simplicity, you can use your own dataset)\n",
    "narrative_text = \"\"\"\n",
    "Sarah was walking down the street when she suddenly bumped into James. \n",
    "\"Oh, sorry!\" she exclaimed. \n",
    "James smiled and said, \"No problem, Sarah.\" \n",
    "\"How have you been?\" Sarah asked, still a bit embarrassed. \n",
    "\"I'm doing great! Just busy with work,\" James replied. \n",
    "\"That's good to hear!\" she said with a smile.\n",
    "\"\"\"\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Use nltk's sent_tokenize to split text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Preprocess the narrative text into sentences\n",
    "sentences = preprocess_text(narrative_text)\n",
    "\n",
    "# Print the sentences\n",
    "for sent in sentences:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sample narrative text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:2475\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m Downloader()\n\u001b[0;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_download_dir()\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:1068\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample narrative text\n",
    "narrative_text = \"\"\"\n",
    "Sarah was walking down the street when she suddenly bumped into James. \n",
    "\"Oh, sorry!\" she exclaimed. \n",
    "James smiled and said, \"No problem, Sarah.\" \n",
    "\"How have you been?\" Sarah asked, still a bit embarrassed. \n",
    "\"I'm doing great! Just busy with work,\" James replied. \n",
    "\"That's good to hear!\" she said with a smile.\n",
    "\"\"\"\n",
    "\n",
    "# Use nltk's sent_tokenize to split text into sentences\n",
    "sentences = sent_tokenize(narrative_text)\n",
    "\n",
    "# Print the sentences\n",
    "for sent in sentences:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sample narrative text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:2475\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m Downloader()\n\u001b[0;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_download_dir()\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:1068\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample narrative text\n",
    "narrative_text = \"\"\"\n",
    "Sarah was walking down the street when she suddenly bumped into James. \n",
    "\"Oh, sorry!\" she exclaimed. \n",
    "James smiled and said, \"No problem, Sarah.\" \n",
    "\"How have you been?\" Sarah asked, still a bit embarrassed. \n",
    "\"I'm doing great! Just busy with work,\" James replied. \n",
    "\"That's good to hear!\" she said with a smile.\n",
    "\"\"\"\n",
    "\n",
    "# Use nltk's sent_tokenize to split text into sentences\n",
    "sentences = sent_tokenize(narrative_text)\n",
    "\n",
    "# Print the sentences\n",
    "for sent in sentences:\n",
    "    print(sent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
